{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70404cef",
      "metadata": {
        "id": "70404cef"
      },
      "outputs": [],
      "source": [
        "import torch, random, itertools, math, json\n",
        "from datasets import load_dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from jiwer import wer\n",
        "import pandas as pd\n",
        "# !pip install jiwer\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f2ed68",
      "metadata": {
        "id": "08f2ed68"
      },
      "source": [
        "# 1. Load three subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf9ef6d",
      "metadata": {
        "id": "2cf9ef6d"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# import csv\n",
        "\n",
        "# lang_ids = {\"hin\": \"hin\", \"tam\": \"tam\", \"ben\": \"ben\"}\n",
        "# output_file = \"train.csv\"\n",
        "\n",
        "# with open(output_file, \"w\", newline='') as csvfile:\n",
        "#     writer = csv.DictWriter(csvfile, fieldnames=[\"roman\", \"native\", \"lang\"])\n",
        "#     writer.writeheader()\n",
        "\n",
        "#     for tag in lang_ids:\n",
        "#         ds = load_dataset(\"ai4bharat/Aksharantar\", streaming=True, split=\"train\")\n",
        "\n",
        "#         def belongs_to_lang(x):\n",
        "#             return x.get(\"unique_identifier\", \"\").startswith(tag)\n",
        "\n",
        "#         filtered = ds.filter(belongs_to_lang)\n",
        "\n",
        "#         count = 0\n",
        "#         for x in filtered:\n",
        "#             try:\n",
        "#                 roman = x[\"english word\"]\n",
        "#                 native = x[\"native word\"]\n",
        "#                 score = x.get(\"score\", 0.0)  # default to 0.0 if missing\n",
        "#                 if score < 0.35:\n",
        "#                     writer.writerow({\n",
        "#                         \"roman\": f\"<LANG_{tag.upper()}> {roman}\",\n",
        "#                         \"native\": native,\n",
        "#                         \"lang\": tag\n",
        "#                     })\n",
        "#                     count += 1\n",
        "#             except Exception:\n",
        "#                 continue\n",
        "\n",
        "#         print(f\"✅ Saved {count} samples for {tag}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "befaa120",
      "metadata": {
        "id": "befaa120"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/pc_draft/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bde59acd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "bde59acd",
        "outputId": "b3aff919-2b74-4ba8-f570-eeeee7620268"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.generic.NDFrame.describe</b><br/>def describe(percentiles=None, include=None, exclude=None) -&gt; Self</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py</a>Generate descriptive statistics.\n",
              "\n",
              "Descriptive statistics include those that summarize the central\n",
              "tendency, dispersion and shape of a\n",
              "dataset&#x27;s distribution, excluding ``NaN`` values.\n",
              "\n",
              "Analyzes both numeric and object series, as well\n",
              "as ``DataFrame`` column sets of mixed data types. The output\n",
              "will vary depending on what is provided. Refer to the notes\n",
              "below for more detail.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "percentiles : list-like of numbers, optional\n",
              "    The percentiles to include in the output. All should\n",
              "    fall between 0 and 1. The default is\n",
              "    ``[.25, .5, .75]``, which returns the 25th, 50th, and\n",
              "    75th percentiles.\n",
              "include : &#x27;all&#x27;, list-like of dtypes or None (default), optional\n",
              "    A white list of data types to include in the result. Ignored\n",
              "    for ``Series``. Here are the options:\n",
              "\n",
              "    - &#x27;all&#x27; : All columns of the input will be included in the output.\n",
              "    - A list-like of dtypes : Limits the results to the\n",
              "      provided data types.\n",
              "      To limit the result to numeric types submit\n",
              "      ``numpy.number``. To limit it instead to object columns submit\n",
              "      the ``numpy.object`` data type. Strings\n",
              "      can also be used in the style of\n",
              "      ``select_dtypes`` (e.g. ``df.describe(include=[&#x27;O&#x27;])``). To\n",
              "      select pandas categorical columns, use ``&#x27;category&#x27;``\n",
              "    - None (default) : The result will include all numeric columns.\n",
              "exclude : list-like of dtypes or None (default), optional,\n",
              "    A black list of data types to omit from the result. Ignored\n",
              "    for ``Series``. Here are the options:\n",
              "\n",
              "    - A list-like of dtypes : Excludes the provided data types\n",
              "      from the result. To exclude numeric types submit\n",
              "      ``numpy.number``. To exclude object columns submit the data\n",
              "      type ``numpy.object``. Strings can also be used in the style of\n",
              "      ``select_dtypes`` (e.g. ``df.describe(exclude=[&#x27;O&#x27;])``). To\n",
              "      exclude pandas categorical columns, use ``&#x27;category&#x27;``\n",
              "    - None (default) : The result will exclude nothing.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "Series or DataFrame\n",
              "    Summary statistics of the Series or Dataframe provided.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.count: Count number of non-NA/null observations.\n",
              "DataFrame.max: Maximum of the values in the object.\n",
              "DataFrame.min: Minimum of the values in the object.\n",
              "DataFrame.mean: Mean of the values.\n",
              "DataFrame.std: Standard deviation of the observations.\n",
              "DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n",
              "    columns based on their dtype.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "For numeric data, the result&#x27;s index will include ``count``,\n",
              "``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n",
              "upper percentiles. By default the lower percentile is ``25`` and the\n",
              "upper percentile is ``75``. The ``50`` percentile is the\n",
              "same as the median.\n",
              "\n",
              "For object data (e.g. strings or timestamps), the result&#x27;s index\n",
              "will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n",
              "is the most common value. The ``freq`` is the most common value&#x27;s\n",
              "frequency. Timestamps also include the ``first`` and ``last`` items.\n",
              "\n",
              "If multiple object values have the highest count, then the\n",
              "``count`` and ``top`` results will be arbitrarily chosen from\n",
              "among those with the highest count.\n",
              "\n",
              "For mixed data types provided via a ``DataFrame``, the default is to\n",
              "return only an analysis of numeric columns. If the dataframe consists\n",
              "only of object and categorical data without any numeric columns, the\n",
              "default is to return an analysis of both the object and categorical\n",
              "columns. If ``include=&#x27;all&#x27;`` is provided as an option, the result\n",
              "will include a union of attributes of each type.\n",
              "\n",
              "The `include` and `exclude` parameters can be used to limit\n",
              "which columns in a ``DataFrame`` are analyzed for the output.\n",
              "The parameters are ignored when analyzing a ``Series``.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Describing a numeric ``Series``.\n",
              "\n",
              "&gt;&gt;&gt; s = pd.Series([1, 2, 3])\n",
              "&gt;&gt;&gt; s.describe()\n",
              "count    3.0\n",
              "mean     2.0\n",
              "std      1.0\n",
              "min      1.0\n",
              "25%      1.5\n",
              "50%      2.0\n",
              "75%      2.5\n",
              "max      3.0\n",
              "dtype: float64\n",
              "\n",
              "Describing a categorical ``Series``.\n",
              "\n",
              "&gt;&gt;&gt; s = pd.Series([&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; s.describe()\n",
              "count     4\n",
              "unique    3\n",
              "top       a\n",
              "freq      2\n",
              "dtype: object\n",
              "\n",
              "Describing a timestamp ``Series``.\n",
              "\n",
              "&gt;&gt;&gt; s = pd.Series([\n",
              "...     np.datetime64(&quot;2000-01-01&quot;),\n",
              "...     np.datetime64(&quot;2010-01-01&quot;),\n",
              "...     np.datetime64(&quot;2010-01-01&quot;)\n",
              "... ])\n",
              "&gt;&gt;&gt; s.describe()\n",
              "count                      3\n",
              "mean     2006-09-01 08:00:00\n",
              "min      2000-01-01 00:00:00\n",
              "25%      2004-12-31 12:00:00\n",
              "50%      2010-01-01 00:00:00\n",
              "75%      2010-01-01 00:00:00\n",
              "max      2010-01-01 00:00:00\n",
              "dtype: object\n",
              "\n",
              "Describing a ``DataFrame``. By default only numeric fields\n",
              "are returned.\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame({&#x27;categorical&#x27;: pd.Categorical([&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;]),\n",
              "...                    &#x27;numeric&#x27;: [1, 2, 3],\n",
              "...                    &#x27;object&#x27;: [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]\n",
              "...                    })\n",
              "&gt;&gt;&gt; df.describe()\n",
              "       numeric\n",
              "count      3.0\n",
              "mean       2.0\n",
              "std        1.0\n",
              "min        1.0\n",
              "25%        1.5\n",
              "50%        2.0\n",
              "75%        2.5\n",
              "max        3.0\n",
              "\n",
              "Describing all columns of a ``DataFrame`` regardless of data type.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(include=&#x27;all&#x27;)  # doctest: +SKIP\n",
              "       categorical  numeric object\n",
              "count            3      3.0      3\n",
              "unique           3      NaN      3\n",
              "top              f      NaN      a\n",
              "freq             1      NaN      1\n",
              "mean           NaN      2.0    NaN\n",
              "std            NaN      1.0    NaN\n",
              "min            NaN      1.0    NaN\n",
              "25%            NaN      1.5    NaN\n",
              "50%            NaN      2.0    NaN\n",
              "75%            NaN      2.5    NaN\n",
              "max            NaN      3.0    NaN\n",
              "\n",
              "Describing a column from a ``DataFrame`` by accessing it as\n",
              "an attribute.\n",
              "\n",
              "&gt;&gt;&gt; df.numeric.describe()\n",
              "count    3.0\n",
              "mean     2.0\n",
              "std      1.0\n",
              "min      1.0\n",
              "25%      1.5\n",
              "50%      2.0\n",
              "75%      2.5\n",
              "max      3.0\n",
              "Name: numeric, dtype: float64\n",
              "\n",
              "Including only numeric columns in a ``DataFrame`` description.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(include=[np.number])\n",
              "       numeric\n",
              "count      3.0\n",
              "mean       2.0\n",
              "std        1.0\n",
              "min        1.0\n",
              "25%        1.5\n",
              "50%        2.0\n",
              "75%        2.5\n",
              "max        3.0\n",
              "\n",
              "Including only string columns in a ``DataFrame`` description.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(include=[object])  # doctest: +SKIP\n",
              "       object\n",
              "count       3\n",
              "unique      3\n",
              "top         a\n",
              "freq        1\n",
              "\n",
              "Including only categorical columns from a ``DataFrame`` description.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(include=[&#x27;category&#x27;])\n",
              "       categorical\n",
              "count            3\n",
              "unique           3\n",
              "top              d\n",
              "freq             1\n",
              "\n",
              "Excluding numeric columns from a ``DataFrame`` description.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(exclude=[np.number])  # doctest: +SKIP\n",
              "       categorical object\n",
              "count            3      3\n",
              "unique           3      3\n",
              "top              f      a\n",
              "freq             1      1\n",
              "\n",
              "Excluding object columns from a ``DataFrame`` description.\n",
              "\n",
              "&gt;&gt;&gt; df.describe(exclude=[object])  # doctest: +SKIP\n",
              "       categorical  numeric\n",
              "count            3      3.0\n",
              "unique           3      NaN\n",
              "top              f      NaN\n",
              "freq             1      NaN\n",
              "mean           NaN      2.0\n",
              "std            NaN      1.0\n",
              "min            NaN      1.0\n",
              "25%            NaN      1.5\n",
              "50%            NaN      2.0\n",
              "75%            NaN      2.5\n",
              "max            NaN      3.0</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 11734);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "<bound method NDFrame.describe of                              roman           native lang\n",
              "0          <LANG_HIN> maitrologist    मैट्रोलॉजिस्ट  hin\n",
              "1                 <LANG_HIN> phwcs  पीएचडब्ल्यूसीएस  hin\n",
              "2        <LANG_HIN> pratidwandiyon  प्रतिद्वन्दियों  hin\n",
              "3            <LANG_HIN> pratiyukti      प्रतियुक्ति  hin\n",
              "4            <LANG_HIN> eksisatens       एक्सिसटेंस  hin\n",
              "...                            ...              ...  ...\n",
              "5143997     <LANG_BEN> chintakarma       চিন্তাকর্ম  ben\n",
              "5143998         <LANG_BEN> elakate          এলাকাতে  ben\n",
              "5143999       <LANG_BEN> asbabgulo        আসবাবগুলো  ben\n",
              "5144000        <LANG_BEN> stantman   স্ট্যান্টম্যান  ben\n",
              "5144001           <LANG_BEN> naogo            নাওগো  ben\n",
              "\n",
              "[5144002 rows x 3 columns]>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6e1a85e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e1a85e",
        "outputId": "50f958f7-6617-4530-9e32-f49f64d52f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['roman', 'native', 'lang'], dtype='object')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "16b5757e",
      "metadata": {
        "id": "16b5757e"
      },
      "outputs": [],
      "source": [
        "# Clean up: ensure strings and remove NaNs\n",
        "df = df[['roman', 'native', 'lang']].dropna()\n",
        "df['roman'] = df['roman'].astype(str)\n",
        "df['native'] = df['native'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d96db8cf",
      "metadata": {
        "id": "d96db8cf"
      },
      "outputs": [],
      "source": [
        "df['lang_token'] = df['roman'].str.extract(r'(<LANG_\\w+>)')\n",
        "df['roman_clean'] = df['roman'].str.replace(r'<LANG_\\w+>\\s*', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3a3d7511",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "3a3d7511",
        "outputId": "c0896874-70dc-4a0f-9a65-e8cb83c4a070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang_token</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_TAM&gt;</th>\n",
              "      <td>3148883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_BEN&gt;</th>\n",
              "      <td>1026393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_HIN&gt;</th>\n",
              "      <td>968726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lang_token\n",
              "<LANG_TAM>    3148883\n",
              "<LANG_BEN>    1026393\n",
              "<LANG_HIN>     968726\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['lang_token'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "NMSlnYmEXyXV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "NMSlnYmEXyXV",
        "outputId": "8fc3f38c-1d1d-4fc8-f848-e42379bc78be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang_token</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_BEN&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_HIN&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_TAM&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lang_token\n",
              "<LANG_BEN>    500000\n",
              "<LANG_HIN>    500000\n",
              "<LANG_TAM>    500000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "sample_size = 500_000\n",
        "random_state = 42\n",
        "import pandas as pd\n",
        "\n",
        "# Sample 500k from each language token\n",
        "df_balanced = pd.concat([\n",
        "    df[df['lang_token'] == '<LANG_HIN>'].sample(n=sample_size, random_state=random_state),\n",
        "    df[df['lang_token'] == '<LANG_BEN>'].sample(n=sample_size, random_state=random_state),\n",
        "    df[df['lang_token'] == '<LANG_TAM>'].sample(n=sample_size, random_state=random_state)\n",
        "], ignore_index=True)\n",
        "\n",
        "# Optional: Shuffle the combined dataset\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "# ✅ Final check\n",
        "df_balanced['lang_token'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "nopMVK0zYKQM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "nopMVK0zYKQM",
        "outputId": "5e90271c-fba8-44fe-b06f-77d5638b8345"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lang_token</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_BEN&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_HIN&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;LANG_TAM&gt;</th>\n",
              "      <td>500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lang_token\n",
              "<LANG_BEN>    500000\n",
              "<LANG_HIN>    500000\n",
              "<LANG_TAM>    500000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df_balanced\n",
        "df['lang_token'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ff97c6c3",
      "metadata": {
        "id": "ff97c6c3"
      },
      "outputs": [],
      "source": [
        "def build_vocab_char(df):\n",
        "    charset = set()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        charset.update(set(row['roman_clean']))   # add all roman characters\n",
        "        charset.update(set(row['native']))        # add all native characters\n",
        "\n",
        "    # ✅ Add all <LANG_XXX> tokens explicitly (not character-by-character!)\n",
        "    charset.update(df['lang_token'].unique())     # this ensures all language tokens are added\n",
        "    print(f\"found land is {df['lang_token'].unique()}\")\n",
        "\n",
        "    charset = sorted(list(charset))\n",
        "\n",
        "    stoi = {c: i + 1 for i, c in enumerate(charset)}  # leave 0 for <pad>\n",
        "    stoi['<pad>'] = 0\n",
        "    stoi['<sos>'] = len(stoi)\n",
        "    stoi['<eos>'] = len(stoi)\n",
        "\n",
        "    itos = {i: c for c, i in stoi.items()}\n",
        "    return stoi, itos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7c78a90b",
      "metadata": {
        "id": "7c78a90b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class MultilingualTranslitDataset(Dataset):\n",
        "    def __init__(self, df, vocab, max_len=40):\n",
        "        self.vocab = vocab\n",
        "        self.samples = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            src = [vocab[row['lang_token']]] + [vocab[c] for c in row['roman_clean'] if c in vocab]\n",
        "            tgt = [vocab['<sos>']] + [vocab[c] for c in row['native'] if c in vocab] + [vocab['<eos>']]\n",
        "\n",
        "            if len(src) <= max_len and len(tgt) <= max_len:\n",
        "                self.samples.append((torch.tensor(src), torch.tensor(tgt)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_pad = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    tgt_pad = pad_sequence(tgts, batch_first=True, padding_value=0)\n",
        "    return src_pad, tgt_pad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8a13a3af",
      "metadata": {
        "id": "8a13a3af"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)  # [B, T] -> [B, T, emb_dim]\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell  # outputs: [B, T, 2*hidden_dim]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, 1)\n",
        "\n",
        "    def forward(self, enc_outputs, dec_hidden):\n",
        "        batch_size, seq_len, enc_dim = enc_outputs.size()\n",
        "        dec_hidden = dec_hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [B, T, H]\n",
        "        energy = self.attn(torch.cat((enc_outputs, dec_hidden), dim=2)).squeeze(2)  # [B, T]\n",
        "        attn_weights = torch.softmax(energy, dim=1)  # [B, T]\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)  # [B, 2*H]\n",
        "        return context\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim + enc_hid_dim * 2, dec_hid_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(dec_hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, enc_outputs):\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # [B] -> [B, 1, emb_dim]\n",
        "        context = self.attention(enc_outputs, hidden[-1])     # [B, 2*enc_hid_dim]\n",
        "        context = context.unsqueeze(1)                        # [B, 1, 2*enc_hid_dim]\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)     # [B, 1, emb+2*H]\n",
        "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))               # [B, output_dim]\n",
        "        return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e33e2c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33e2c3c",
        "outputId": "fa4793e1-2eb2-4063-fc20-dc0718920d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found land is ['<LANG_BEN>' '<LANG_HIN>' '<LANG_TAM>']\n"
          ]
        }
      ],
      "source": [
        "vocab, inv_vocab = build_vocab_char(df)\n",
        "\n",
        "dataset = MultilingualTranslitDataset(df, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define encoder/decoder\n",
        "vocab_size = len(vocab)\n",
        "encoder = Encoder(input_dim=vocab_size, emb_dim=128, hidden_dim=256)\n",
        "decoder = Decoder(output_dim=vocab_size, emb_dim=128, enc_hid_dim=256, dec_hid_dim=256)\n",
        "\n",
        "# Training loop same as before\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "x-7viomQT3Zp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-7viomQT3Zp",
        "outputId": "e06a2d4c-2a5d-46c9-f1da-4ee3ecec4641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<LANG_BEN>': 1, '<LANG_HIN>': 2, '<LANG_TAM>': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, 'ँ': 30, 'ं': 31, 'ः': 32, 'अ': 33, 'आ': 34, 'इ': 35, 'ई': 36, 'उ': 37, 'ऊ': 38, 'ऋ': 39, 'ए': 40, 'ऐ': 41, 'ऑ': 42, 'ओ': 43, 'औ': 44, 'क': 45, 'ख': 46, 'ग': 47, 'घ': 48, 'ङ': 49, 'च': 50, 'छ': 51, 'ज': 52, 'झ': 53, 'ञ': 54, 'ट': 55, 'ठ': 56, 'ड': 57, 'ढ': 58, 'ण': 59, 'त': 60, 'थ': 61, 'द': 62, 'ध': 63, 'न': 64, 'प': 65, 'फ': 66, 'ब': 67, 'भ': 68, 'म': 69, 'य': 70, 'र': 71, 'ल': 72, 'ळ': 73, 'व': 74, 'श': 75, 'ष': 76, 'स': 77, 'ह': 78, '़': 79, 'ा': 80, 'ि': 81, 'ी': 82, 'ु': 83, 'ू': 84, 'ृ': 85, 'ॅ': 86, 'े': 87, 'ै': 88, 'ॉ': 89, 'ॊ': 90, 'ो': 91, 'ौ': 92, '्': 93, 'ঁ': 94, 'ং': 95, 'ঃ': 96, 'অ': 97, 'আ': 98, 'ই': 99, 'ঈ': 100, 'উ': 101, 'ঊ': 102, 'ঋ': 103, 'এ': 104, 'ঐ': 105, 'ও': 106, 'ঔ': 107, 'ক': 108, 'খ': 109, 'গ': 110, 'ঘ': 111, 'ঙ': 112, 'চ': 113, 'ছ': 114, 'জ': 115, 'ঝ': 116, 'ঞ': 117, 'ট': 118, 'ঠ': 119, 'ড': 120, 'ঢ': 121, 'ণ': 122, 'ত': 123, 'থ': 124, 'দ': 125, 'ধ': 126, 'ন': 127, 'প': 128, 'ফ': 129, 'ব': 130, 'ভ': 131, 'ম': 132, 'য': 133, 'র': 134, 'ল': 135, 'শ': 136, 'ষ': 137, 'স': 138, 'হ': 139, '়': 140, 'া': 141, 'ি': 142, 'ী': 143, 'ু': 144, 'ূ': 145, 'ৃ': 146, 'ে': 147, 'ৈ': 148, 'ো': 149, 'ৌ': 150, '্': 151, 'ৎ': 152, 'ஃ': 153, 'அ': 154, 'ஆ': 155, 'இ': 156, 'ஈ': 157, 'உ': 158, 'ஊ': 159, 'எ': 160, 'ஏ': 161, 'ஐ': 162, 'ஒ': 163, 'ஓ': 164, 'க': 165, 'ங': 166, 'ச': 167, 'ஜ': 168, 'ஞ': 169, 'ட': 170, 'ண': 171, 'த': 172, 'ந': 173, 'ன': 174, 'ப': 175, 'ம': 176, 'ய': 177, 'ர': 178, 'ற': 179, 'ல': 180, 'ள': 181, 'ழ': 182, 'வ': 183, 'ஷ': 184, 'ஸ': 185, 'ஹ': 186, 'ா': 187, 'ி': 188, 'ீ': 189, 'ு': 190, 'ூ': 191, 'ெ': 192, 'ே': 193, 'ை': 194, 'ொ': 195, 'ோ': 196, 'ௌ': 197, '்': 198, '<pad>': 0, '<sos>': 199, '<eos>': 200}\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "93242d3e",
      "metadata": {
        "id": "93242d3e"
      },
      "outputs": [],
      "source": [
        "def predict_multilingual(word_with_lang_token, encoder, decoder, vocab, itos, device, max_len=40):\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = word_with_lang_token.strip().split()\n",
        "        lang_token = tokens[0]\n",
        "        word = ' '.join(tokens[1:])\n",
        "        src_seq = [vocab[lang_token]] + [vocab[c] for c in word if c in vocab]\n",
        "        src = torch.tensor(src_seq).unsqueeze(0).to(device)\n",
        "\n",
        "        enc_out, h, c = encoder(src)\n",
        "\n",
        "        # Merge bi-directional encoder states\n",
        "        h = h[0:h.size(0):2] + h[1:h.size(0):2]\n",
        "        c = c[0:c.size(0):2] + c[1:c.size(0):2]\n",
        "\n",
        "        dec_input = torch.tensor([vocab['<sos>']], device=device)\n",
        "        preds = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, h, c = decoder(dec_input, h, c, enc_out)\n",
        "            pred_token = output.argmax(1).item()\n",
        "            if itos.get(pred_token) == '<eos>':\n",
        "                break\n",
        "            preds.append(pred_token)\n",
        "            dec_input = torch.tensor([pred_token], device=device)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"Inference took {duration:.2f} seconds\")\n",
        "    return ''.join(itos[i] for i in preds if i in itos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "de494eda",
      "metadata": {
        "id": "de494eda"
      },
      "outputs": [],
      "source": [
        "from jiwer import cer, wer\n",
        "\n",
        "def evaluate_multilang(df, encoder, decoder, vocab, itos, device):\n",
        "    results = []\n",
        "    for lang in df['lang'].unique():\n",
        "        lang_df = df[df['lang'] == lang].sample(n=200)  # Sample subset\n",
        "        total_cer, total_wer = 0, 0\n",
        "        for _, row in lang_df.iterrows():\n",
        "            input_seq = f\"{row['lang_token']} {row['roman_clean']}\"\n",
        "            pred = predict_multilingual(input_seq, encoder, decoder, vocab, itos, device)\n",
        "            cer_score = cer(row['native'], pred)\n",
        "            wer_score = wer(row['native'], pred)\n",
        "            total_cer += cer_score\n",
        "            total_wer += wer_score\n",
        "        avg_cer = total_cer / len(lang_df)\n",
        "        avg_wer = total_wer / len(lang_df)\n",
        "        results.append((lang, avg_cer, avg_wer))\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NyA2txZyZXhs",
      "metadata": {
        "id": "NyA2txZyZXhs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "35a248d1",
      "metadata": {
        "id": "35a248d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def train(encoder, decoder, dataloader, vocab, criterion, optimizer, device='cpu', num_epochs=10, checkpoint_dir=\"checkpoints\", start_epoch=0):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (src_batch, tgt_batch) in enumerate(progress_bar):\n",
        "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            enc_outputs, h, c = encoder(src_batch)\n",
        "\n",
        "            # Merge bidirectional states\n",
        "            h = h[::2] + h[1::2]\n",
        "            c = c[::2] + c[1::2]\n",
        "\n",
        "            dec_input = tgt_batch[:, 0]\n",
        "            loss = 0\n",
        "\n",
        "            for t in range(1, tgt_batch.size(1)):\n",
        "                output, h, c = decoder(dec_input, h, c, enc_outputs)\n",
        "                loss += criterion(output, tgt_batch[:, t])\n",
        "                dec_input = tgt_batch[:, t]\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"\\nEpoch {epoch+1} finished in {epoch_time:.2f}s | Total Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # ✅ Save checkpoint\n",
        "        checkpoint_path = os.path.join(\"/content/drive/MyDrive/pc_draft\", f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'vocab': vocab\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        print(f\"✅ Saved checkpoint to {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "zE61GnfsThmC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE61GnfsThmC",
        "outputId": "3d46b990-d768-4b27-d4eb-9fea4489be7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found: <LANG_BEN>\n",
            "Found: <LANG_HIN>\n",
            "Found: <LANG_TAM>\n"
          ]
        }
      ],
      "source": [
        "for key in vocab.keys():\n",
        "    if key.startswith(\"<LANG\"):\n",
        "        print(\"Found:\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b24208b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b24208b5",
        "outputId": "36137ebd-86d9-4ea2-de2c-9b23ec20cc67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 23438/23438 [18:28<00:00, 21.15it/s, loss=2.06]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 finished in 1108.29s | Total Loss: 114475.3078\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 23438/23438 [18:19<00:00, 21.32it/s, loss=1.84]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 finished in 1099.20s | Total Loss: 54512.2484\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_2.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 23438/23438 [18:11<00:00, 21.47it/s, loss=1.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 finished in 1091.69s | Total Loss: 46772.6660\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_3.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 23438/23438 [18:04<00:00, 21.62it/s, loss=2.97]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 finished in 1084.18s | Total Loss: 42918.3472\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 23438/23438 [18:04<00:00, 21.62it/s, loss=1.22]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 finished in 1084.04s | Total Loss: 40292.9706\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_5.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 23438/23438 [18:06<00:00, 21.57it/s, loss=1.13]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 finished in 1086.60s | Total Loss: 38538.0547\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 23438/23438 [18:12<00:00, 21.46it/s, loss=2.68]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 finished in 1092.37s | Total Loss: 37331.7633\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_7.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 23438/23438 [18:31<00:00, 21.09it/s, loss=1.91]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 finished in 1111.34s | Total Loss: 36290.7890\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 23438/23438 [18:02<00:00, 21.65it/s, loss=2.31]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 finished in 1082.74s | Total Loss: 35340.5804\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_9.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 23438/23438 [18:04<00:00, 21.62it/s, loss=2.07]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 finished in 1084.15s | Total Loss: 34698.9414\n",
            "✅ Saved checkpoint to /content/drive/MyDrive/pc_draft/checkpoint_epoch_10.pth\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train(encoder, decoder, dataloader, vocab, criterion, optimizer, device=device, num_epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s5IT2MT_9gqS",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s5IT2MT_9gqS",
        "outputId": "3d84ebc0-8c26-4f98-dee9-5586a7a6c97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5cd35779",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cd35779",
        "outputId": "6876f738-a5e5-48fd-ab5e-4edbae55823d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model saved.\n"
          ]
        }
      ],
      "source": [
        "itos = {i: ch for ch, i in vocab.items()}\n",
        "\n",
        "torch.save({\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict(),\n",
        "    'vocab': vocab,\n",
        "    'itos': itos,\n",
        "}, \"/content/drive/MyDrive/pc_draft/translit_model_checkpoint.pth\")\n",
        "print(\"✅ Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fcb603",
      "metadata": {
        "id": "e9fcb603"
      },
      "outputs": [],
      "source": [
        "def predict_multilingual(word_with_lang_token, encoder, decoder, vocab, itos, device, max_len=40):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = word_with_lang_token.strip().split()\n",
        "        lang_token = tokens[0]\n",
        "        word = ' '.join(tokens[1:])\n",
        "        src_seq = [vocab[lang_token]] + [vocab[c] for c in word if c in vocab]\n",
        "        src = torch.tensor(src_seq).unsqueeze(0).to(device)\n",
        "\n",
        "        enc_out, h, c = encoder(src)\n",
        "        dec_input = torch.tensor([vocab['<sos>']], device=device)\n",
        "        preds = []\n",
        "        for _ in range(max_len):\n",
        "            output, h, c = decoder(dec_input, h, c, enc_out)\n",
        "            pred_token = output.argmax(1).item()\n",
        "            if itos[pred_token] == '<eos>':\n",
        "                break\n",
        "            preds.append(pred_token)\n",
        "            dec_input = torch.tensor([pred_token], device=device)\n",
        "        return ''.join(itos[i] for i in preds if i in itos)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
